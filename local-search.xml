<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>nano-vllm复现</title>
    <link href="/2025/12/05/nano-vllm%E5%A4%8D%E7%8E%B0/"/>
    <url>/2025/12/05/nano-vllm%E5%A4%8D%E7%8E%B0/</url>
    
    <content type="html"><![CDATA[<p>项目链接<a href="https://github.com/GeeeekExplorer/nano-vllm">https://github.com/GeeeekExplorer/nano-vllm</a></p><p>project.toml中的dependencies如下：</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">dependencies</span> = [<br>    <span class="hljs-string">&quot;torch&gt;=2.4.0&quot;</span>,<br>    <span class="hljs-string">&quot;triton&gt;=3.0.0&quot;</span>,<br>    <span class="hljs-string">&quot;transformers&gt;=4.51.0&quot;</span>,<br>    <span class="hljs-string">&quot;flash-attn&quot;</span>,<br>    <span class="hljs-string">&quot;xxhash&quot;</span>,<br>]<br></code></pre></td></tr></table></figure><h1 id="安装pytorch和flash-attn"><a href="#安装pytorch和flash-attn" class="headerlink" title="安装pytorch和flash-attn"></a>安装pytorch和flash-attn</h1><p>由于xxhash&#x2F;transformers&#x2F;triton的wheel原封不动在PyPI，所以安装nano-vllm时会自动补齐，只需要安装torch和flash-attn。</p><p>用miniconda创建虚拟环境：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">source</span> ~/miniconda3/etc/profile.d/conda.sh <span class="hljs-comment"># 初始化conda</span><br>conda create -n nanovllm python=3.10 -y <span class="hljs-comment"># 创建虚拟环境</span><br>conda activate nanovllm <span class="hljs-comment"># 激活环境</span><br></code></pre></td></tr></table></figure><p>还有两个命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">conda <span class="hljs-built_in">env</span> list <span class="hljs-comment"># 查看虚拟环境</span><br>conda deactivate <span class="hljs-comment">#退出当前虚拟环境</span><br></code></pre></td></tr></table></figure><p>然后检查NVIDIA GPU：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">nvidia-smi<br></code></pre></td></tr></table></figure><p>如图:</p><img src="/2025/12/05/nano-vllm%E5%A4%8D%E7%8E%B0/0.jpg" class=""> <p>可以看到驱动版本是580.65.06(Driver ≥ 535 → 用 CUDA 12.1,Driver 470–530 → 用 CUDA 11.8)，于是用以下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">conda install pytorch pytorch-cuda=12.1 -c pytorch -c nvidia -y<br></code></pre></td></tr></table></figure><p>使用readme中的<code>pip install git+https://github.com/GeeeekExplorer/nano-vllm.git</code>会报错，改为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> https://github.com/GeeeekExplorer/nano-vllm.git<br><span class="hljs-built_in">cd</span> nano-vllm<br>pip install -e .<br></code></pre></td></tr></table></figure><p>这时报错<code>libtorch_cpu.so: undefined symbol: iJIT_NotifyEvent</code>，搜了一下是由于 MKL（Intel Math Kernel Library）版本不匹配导致的，通过 pip 安装的 PyTorch 二进制文件是静态链接到 MKL 的，可以避免这个错误：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip uninstall torch torchvision torchaudio -y<br>conda uninstall pytorch pytorch-cuda -y<br>pip install torch torchvision torchaudio<br>python -c <span class="hljs-string">&quot;import torch; print(torch.__version__, torch.cuda.is_available())&quot;</span> <span class="hljs-comment">#看到true即可</span><br></code></pre></td></tr></table></figure><p>这时可以看到安装成功了，但是GitHub Release页面里没有torch2.9的flash-attn预编译包，由于不想自己源码编译flash-attn，于是对pytorch进行降级：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip uninstall torch torchvision torchaudio -y<br><span class="hljs-comment"># 版本 2.5.1;预编译的 CUDA 12.1 版本</span><br>pip install torch==2.5.1+cu121 torchvision torchaudio \<br>            --extra-index-url https://download.pytorch.org/whl/cu121<br>python -c <span class="hljs-string">&quot;import torch; print(torch.__version__, torch.cuda.is_available())&quot;</span> <span class="hljs-comment">#2.5.1+cu121 True</span><br></code></pre></td></tr></table></figure><p>然后安装flash-attn,一开始显示glibc版本不匹配，然后参考了这个博客<a href="https://jishuzhan.net/article/1943972108803026946%EF%BC%8C%E9%87%87%E7%94%A8%E7%89%88%E6%9C%AC2.7.4.post1%EF%BC%8C%E6%9C%80%E5%90%8E%E4%BD%BF%E7%94%A8%E5%A6%82%E4%B8%8B%E5%91%BD%E4%BB%A4%EF%BC%88cxx11abiTRUE%EF%BC%9A%E7%94%A8">https://jishuzhan.net/article/1943972108803026946，采用版本2.7.4.post1，最后使用如下命令（cxx11abiTRUE：用</a> 新 C++11 ABI，使用glibc ≥ 2.17；cu12 + torch2.5 + cp310）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">wget https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.3/flash_attn-2.7.4.post1+cu12torch2.5cxx11abiFALSE-cp310-cp310-linux_x86_64.whl<br>pip install flash_attn-2.7.4.post1+cu12torch2.5cxx11abiFALSE-cp310-cp310-linux_x86_64.whl<br>python -c <span class="hljs-string">&quot;import flash_attn, torch; print(flash_attn.__version__, torch.cuda.is_available())&quot;</span> <span class="hljs-comment"># 验证</span><br></code></pre></td></tr></table></figure><h1 id="安装nano-vllm"><a href="#安装nano-vllm" class="headerlink" title="安装nano-vllm"></a>安装nano-vllm</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> nano-vllm<br>pip install -e .<br></code></pre></td></tr></table></figure><p>然后下载权重模型，由于服务器无法ping通huggingface.co，这里使用的是镜像下载：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 设置 Hugging Face 镜像</span><br><span class="hljs-built_in">export</span> HF_ENDPOINT=https://hf-mirror.com<br><span class="hljs-comment"># 然后下载</span><br>huggingface-cli download --resume-download Qwen/Qwen3-0.6B \<br>  --local-dir ~/huggingface/Qwen3-0.6B/ \<br>  --local-dir-use-symlinks False<br></code></pre></td></tr></table></figure><p>运行example.py,<code>python example.py</code>,结果如图：</p><img src="/2025/12/05/nano-vllm%E5%A4%8D%E7%8E%B0/1.jpg" class=""> <p>运行bench.py,<code>python bench.py</code>,结果如图：</p><img src="/2025/12/05/nano-vllm%E5%A4%8D%E7%8E%B0/2.jpg" class=""><p>速度和吞吐率都比readme中的快很多，应该是由于显卡是A100。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>OpenCAEPoro安装</title>
    <link href="/2025/12/04/OpenCAEPoro%E5%AE%89%E8%A3%85/"/>
    <url>/2025/12/04/OpenCAEPoro%E5%AE%89%E8%A3%85/</url>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本文记录了在学校服务器（CentOS8）上安装OpenCAEPoro和切换编译器遇到的问题和解决方法。</p><h1 id="一、准备工作"><a href="#一、准备工作" class="headerlink" title="一、准备工作"></a>一、准备工作</h1><p>在服务器上安装intel-oneapi-hpc-toolkit，其中包含icx，由于没有root权限，所以我们下载离线安装包进行本地安装，在<code>https://www.intel.com/content/www/us/en/developer/tools/oneapi/hpc-toolkit-download.html</code><br>选Linux → Offline，复制给出的链接下载本地安装包，然后使用它给出的命令直接安装即可：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">wget https://registrationcenter-download.intel.com/akdlm/IRC_NAS/66021d90-934d-41f4-bedf-b8c00bbe98bc/intel-oneapi-hpc-toolkit-2025.3.0.381_offline.sh<br>sh ./intel-oneapi-hpc-toolkit-2025.3.0.381_offline.sh -a --silent --cli --eula accept<br></code></pre></td></tr></table></figure><p>安装后检查一下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">source</span> ~/intel/oneapi/setvars.sh <span class="hljs-comment"># 激活环境</span><br><span class="hljs-built_in">which</span> icx<br></code></pre></td></tr></table></figure><p>为了方便后续使用,指定ROOT_DIR的路径,写进.bashrc文件中：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> ROOT_DIR=/home/zh/OpenCAEPoro_ASC2024-main <span class="hljs-comment"># 根据自己的目录修改</span><br></code></pre></td></tr></table></figure><p>拉取仓库<a href="https://github.com/OpenCAEPlus/OpenCAEPoro_ASC2024%E5%90%8E%EF%BC%8C%E5%B0%86%E5%85%AD%E4%B8%AA%E5%8E%8B%E7%BC%A9%E5%8C%85%E5%85%A8%E9%83%A8%E8%A7%A3%E5%8E%8B%EF%BC%9A">https://github.com/OpenCAEPlus/OpenCAEPoro_ASC2024后，将六个压缩包全部解压：</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">tar -xzf OpenCAEPoro.tar.gz <span class="hljs-comment"># 其他五个同理</span><br></code></pre></td></tr></table></figure><h1 id="二、安装Lapack"><a href="#二、安装Lapack" class="headerlink" title="二、安装Lapack"></a>二、安装Lapack</h1><p>进入lapack-3.11目录下，按照readme中命令编译lapack：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">S2. make blaslib<br>S3. make cblaslib<br>S4. make lapacklib<br>S5. make lapackelib<br></code></pre></td></tr></table></figure><h1 id="三、安装parmetis"><a href="#三、安装parmetis" class="headerlink" title="三、安装parmetis"></a>三、安装parmetis</h1><p>由于MPI还是2021.17旧版，它的mpiicx脚本里硬编码写死icc&#x2F;icpc&#x2F;ifort，需要告诉 MPI 包装器：后端编译器换成 icx&#x2F;icpx&#x2F;ifx：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> I_MPI_CC=icx  <span class="hljs-comment"># 写进.bashrc</span><br><span class="hljs-built_in">export</span> I_MPI_CXX=icpx<br><span class="hljs-built_in">export</span> I_MPI_FC=ifx<br></code></pre></td></tr></table></figure><p>进入lapack-3.11目录下,修改build-parmetis.sh：</p><img src="/2025/12/04/OpenCAEPoro%E5%AE%89%E8%A3%85/build-parmetis.png" class=""><p>同时修改Makefile，只需把<code>cc=mpicc</code>修改为<code>cc=mpiicx</code>，然后执行脚本即可：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sh build-parmetis.sh<br></code></pre></td></tr></table></figure><h1 id="四、安装Hypre"><a href="#四、安装Hypre" class="headerlink" title="四、安装Hypre"></a>四、安装Hypre</h1><p>进入hypre-2.28.0目录,修改build-hypre.sh：</p><img src="/2025/12/04/OpenCAEPoro%E5%AE%89%E8%A3%85/build-hypre.png" class=""><p>注意除了修改路径外，还要禁用Fortran<code>–-disable-fortran</code>，因为旧版 Intel MPI 2021.17 的 mpiifort 包装器里硬编码调用 ifort，用<code>export I_MPI_FC=ifx</code>也不行，系统里只有 oneAPI 2025.3 的 ifx，而Hypre本身完全是C&#x2F;C++写的，fortran是方便跟用户 Fortran 程序联用，所以我就直接禁用了fortran。好像还可以用oneAPI 2021 旧版离线包（含 ifort）本地安装，但就没尝试了。</p><p>然后执行脚本即可：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sh build-hypre.sh<br></code></pre></td></tr></table></figure><h1 id="五、安装petsc"><a href="#五、安装petsc" class="headerlink" title="五、安装petsc"></a>五、安装petsc</h1><p>进入pets目录，修改build-petsc脚本，如图：</p><img src="/2025/12/04/OpenCAEPoro%E5%AE%89%E8%A3%85/build-petsc.png" class=""><p>除了修改路径外，还有多了<code>--with-fc=0 \</code>来禁用fortran，以及</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">-Wno-error=missing-template-arg-list-after-template-kw<br></code></pre></td></tr></table></figure><p>这是由于这是 C++20 新语法检查变严格，而 PETSc 3.19 还没改，而icx 2025.3 默认开启<code>-Werror=missing-template-arg-list-after-template-kw</code>,于是给icx&#x2F;icpx 降诊断等级，让它别报这条错误。还可以用 icx 2023&#x2F;2024（默认不开启这条警告），但没有安装。</p><p>这时执行脚本还是会报错：PETSc 自带的 MPI 自检例 (ex19) 在运行时找不到 InfiniBand 网卡，于是 MPI_Init 失败，程序 abort，可以执行如下命令让 MPI 回退到TCP <code>export I_MPI_OFI_PROVIDER=tcp</code>，然后再执行脚本就可以了。</p><h1 id="六、安装petsc-solver"><a href="#六、安装petsc-solver" class="headerlink" title="六、安装petsc_solver"></a>六、安装petsc_solver</h1><p>进入petsc_solver，修改build-petscsolver.sh:</p><img src="/2025/12/04/OpenCAEPoro%E5%AE%89%E8%A3%85/build-petscsolver.png" class=""><p>修改 CMakeList.txt（只用改set那行的路径）:</p><img src="/2025/12/04/OpenCAEPoro%E5%AE%89%E8%A3%85/CMakeList.png" class=""><p>执行脚本即可。</p><h1 id="七、编译OpenCAEPoro"><a href="#七、编译OpenCAEPoro" class="headerlink" title="七、编译OpenCAEPoro"></a>七、编译OpenCAEPoro</h1><p>进入到 OpenCAEPoro 目录，修改mpi-build-petsc.sh：</p><img src="/2025/12/04/OpenCAEPoro%E5%AE%89%E8%A3%85/mpi-build-petsc.png" class=""><p>执行脚本即可。</p><h1 id="八、切换编译器"><a href="#八、切换编译器" class="headerlink" title="八、切换编译器"></a>八、切换编译器</h1><h2 id="切换gcc"><a href="#切换gcc" class="headerlink" title="切换gcc"></a>切换gcc</h2><p>使用 <code>./opt/rh/gcc-toolset-13/enable</code>切换gcc版本到13（gcc默认系统版本较低，不支持c++20），然后再执行：</p><ul><li>重新编译parmetis<br>直接按刚才方法修改编译器会报错，这是由于ParMETIS 的 Makefile 不会自动把 -lmpi 加进链接器，而我的系统 MPI 是 Intel MPI 2021.17，它的库不在默认搜索路径里，进入脚本在make那行加入<code>LDFLAGS=&quot;$LDFLAGS&quot;</code>，然后执行：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">rm</span> -rf build Makefile.inc <span class="hljs-comment"># 删除build和配置文件</span><br><span class="hljs-built_in">export</span> LDFLAGS=<span class="hljs-string">&quot;-L<span class="hljs-variable">$HOME</span>/intel/oneapi/mpi/2021.17/lib -lmpi -lmpifort -lrt -lpthread -ldl&quot;</span> <span class="hljs-comment"># 因为which mpicc显示~/intel/oneapi/mpi/2021.17/bin/mpicc</span><br>sh build-parmetis.sh<br></code></pre></td></tr></table></figure></li><li>重新编译petsc<br>修改脚本中的编译器cc&#x3D;mpicc,cxx&#x3D;mpicxx即可，然后执行脚本。</li><li>重新编译petsc_solver<br>同样修改脚本的编译器即可重新编译即可。</li><li>重新编译OpenCAEPoro<br>同刚才两个一样。</li></ul><h2 id="切换nvc"><a href="#切换nvc" class="headerlink" title="切换nvc"></a>切换nvc</h2><p>   安装NVIDIA HPC SDK：<br>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">wget https://developer.download.nvidia.com/hpc-sdk/24.11/nvhpc_2024_2411_Linux_x86_64_cuda_12.6.tar.gz<br>tar -xzf nvhpc_2024_2411_Linux_x86_64_cuda_12.6.tar.gz<br>./install <span class="hljs-comment"># 选auto install然后指定目录</span><br></code></pre></td></tr></table></figure><br>  验证一下：<br>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">NVARCH=$(<span class="hljs-built_in">uname</span> -s)_$(<span class="hljs-built_in">uname</span> -m)          <span class="hljs-comment"># Linux_x86_64</span><br>NVHOME=<span class="hljs-variable">$HOME</span>/nvhpc-24.11                <span class="hljs-comment"># 你刚才的 --prefix</span><br><span class="hljs-built_in">export</span> PATH=<span class="hljs-variable">$NVHOME</span>/<span class="hljs-variable">$NVARCH</span>/24.11/compilers/bin:<span class="hljs-variable">$PATH</span><br><span class="hljs-built_in">export</span> MANPATH=<span class="hljs-variable">$NVHOME</span>/<span class="hljs-variable">$NVARCH</span>/24.11/compilers/man:<span class="hljs-variable">$MANPATH</span><br>nvc -V<br>nvfortran -V<br></code></pre></td></tr></table></figure><br>  重新编译parmetis、petsc、petsc_solver、OpenCAEPoro，在gcc基础上改成cc&#x3D;nvc、cxx&#x3D;nvc++即可,但要把<code>-Werror=missing-template-arg-list-after-template-kw</code>,因为nvc识别不了。</p><h1 id="九、修改编译选项"><a href="#九、修改编译选项" class="headerlink" title="九、修改编译选项"></a>九、修改编译选项</h1><p>可以在petsc修改编译选项，修改build-petsc脚本，然后再执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">rm</span> -rf build Makefile.inc   <span class="hljs-comment"># 切换到pets目录后</span><br>sh build-petsc.sh<br><span class="hljs-built_in">rm</span> -rf build <span class="hljs-comment"># 切换到petsc_solver后</span><br>sh build-petscsolver.sh<br><span class="hljs-built_in">rm</span> -rf build <span class="hljs-comment"># 切换到OpenCAEPoro目录后</span><br>sh mpi-build-petsc.sh<br></code></pre></td></tr></table></figure><p>进入OpenCAEPoro主目录之后运行测试命令和baseline：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">mpirun -n p ./testOpenCAEPoro ./data/test/test.data <span class="hljs-comment"># 测试命令</span><br>mpirun -np p ./testOpenCAEPoro ./data/case1/case1.data verbose=1 <span class="hljs-comment"># baseline</span><br></code></pre></td></tr></table></figure><p>以下测试均是<code>mpirun -np 45 ./testOpenCAEPoro ./data/case1/case1.data verbose=1</code></p><ul><li>icx编译选项</li></ul><table><thead><tr><th>编译选项</th><th>时间</th></tr></thead><tbody><tr><td>-O0</td><td>198.353</td></tr><tr><td>-O1</td><td>171.759</td></tr><tr><td>-O2</td><td>159.783</td></tr><tr><td>-O3</td><td>167.469</td></tr></tbody></table><ul><li>gcc编译选项<br>首先测试了常见的几种，结果如下：</li></ul><table><thead><tr><th>编译选项</th><th>时间</th><th>备注</th></tr></thead><tbody><tr><td>-O0</td><td>198.406</td><td>关闭优化，调试默认</td></tr><tr><td>-O1</td><td>172.02</td><td>平衡优化与编译速度</td></tr><tr><td>-O2</td><td>163.994</td><td>发布版推荐，95% 场景够用</td></tr><tr><td>-O3</td><td>174.554</td><td>激进优化（循环展开、向量化、内联翻倍）</td></tr><tr><td>-Ofast</td><td>174.61</td><td>相当于-O3 + -ffast-math</td></tr></tbody></table><p>  可以看到O2的时间最少，接下来以O2为基础，然后添加了三个常用选项，作用如下：<br>  <code>-march=native</code>:自动启用本机最新指令集（AVX2&#x2F;AVX-512），替代Intel的-xHost。<br>  <code>-mtune=native</code>:调度策略针对本机,但可移植性差。<br>  &#96;-flto: Link Time Optimizatio，链接时优化,生成更小、更快的代码。</p><p>  时间为170.393s，有所增加，然后删除-flto，时间为166.855s，与不加-march&#x3D;native和-mtune&#x3D;native差不多。</p><p>  接下来选用<code>-ffast-math</code>选项（启用激进浮点数学优化），时间为174.084s，增加较多，删掉然后再添加<code>-funroll-loops</code>（执行循环展开优化），时间为169.185s。</p><ul><li>nvc编译选项</li></ul><table><thead><tr><th>编译选项</th><th>时间</th></tr></thead><tbody><tr><td>-O0</td><td>203.112</td></tr><tr><td>-O1</td><td>190.596</td></tr><tr><td>-O2</td><td>176.914</td></tr><tr><td>-O3</td><td>183.758</td></tr><tr><td>-O4</td><td>179.996</td></tr><tr><td>-fast</td><td>177.206</td></tr></tbody></table>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>测试文章</title>
    <link href="/2025/11/30/%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/"/>
    <url>/2025/11/30/%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/</url>
    
    <content type="html"><![CDATA[<img src="/2025/11/30/%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/test.png" class="" title="图片引用方法一">]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
